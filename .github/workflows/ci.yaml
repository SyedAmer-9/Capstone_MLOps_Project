name: CI Pipeline

on: push

jobs:
  model-training:
    name: 1. Model Training & Metrics
    runs-on: ubuntu-latest
    
    env:  
      DAGSHUB_USER_TOKEN: ${{ secrets.DAGSHUB_USER_TOKEN }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Python & Dependencies
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: pip install -r requirements.txt
      
      - name: Configure AWS for DVC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_DEFAULT_REGION }}

      - name: Set up DVC
        uses: iterative/setup-dvc@v1

      - name: Pull DVC Data
        run: dvc pull

      - name: Run DVC Pipeline (Ingest, Feat Eng, Train, Eval, Register)
        run: dvc repro
        env:
          # Inject token for model_evaluation.py to register model
          DAGSHUB_USER_TOKEN: ${{ secrets.DAGSHUB_USER_TOKEN }}
          
      - name: DVC Push Artifacts (Secure)
        # This explicitly uses the configured credentials to talk to S3
        run: dvc push
        env:
          # Inject the AWS secrets directly into the DVC push environment
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}

      # --- CRITICAL FIX: Upload Vectorizer as Artifact ---
      # This is the necessary backup to ensure the image builds even if S3 fails
      - name: Upload Vectorizer for Docker Build
        uses: actions/upload-artifact@v4
        with:
          name: vectorizer-artifact
          path: models/vectorizer.pkl
        
      - name: Save Metrics Artifact for Testing
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: model-metrics
          path: reports/metrics.json
      - name: Pull DVC artifacts
        run: |
          dvc pull models/vectorizer.pkl
  # -----------------------------------------------
  # Job 2: build-image (CD Stage - Application Packaging)
  # -----------------------------------------------
  build-image:
    name: 2. Build & Push Docker Image
    needs: model-training
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      # -----------------------------------------------------------------
      # NEW STEP: Configure AWS for Docker Build
      # This is crucial. It injects the keys into the *runner's environment*
      # so the 'docker build' command can run 'dvc pull' inside the container.
      # -----------------------------------------------------------------
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_DEFAULT_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2
        
      - name: Build and Push Image (DVC Pull happens during build)
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          ECR_REPOSITORY: ${{ secrets.ECR_REPOSITORY }}
          IMAGE_TAG: latest
        run: |
          # Build: DVC inside the Dockerfile handles pulling the vectorizer from S3.
          docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
          echo "image=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG" >> $GITHUB_OUTPUT
  # -----------------------------------------------
  # Job 3: deploy-to-eks (CD Stage - Deployment)
  # -----------------------------------------------
  deploy-to-eks:
    name: 3. Deploy to EKS Cluster
    needs: build-image
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Install kubectl (for EKS communication)
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Configure AWS EKS Credentials (via aws eks update-kubeconfig)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_DEFAULT_REGION }}
          
      - name: Update kubeconfig
        run: |
          # This command authenticates the runner to the EKS cluster
          aws eks update-kubeconfig --region ${{ secrets.AWS_DEFAULT_REGION }} --name flask-app-cluster
      
      # -----------------------------------------------------------------
      # THE FIX: Create Kubernetes Secret
      # We must create a secret object for the token.
      # -----------------------------------------------------------------
      - name: Create Kubernetes Secret
        run: |
          kubectl create secret generic dagshub-token \
            --from-literal=DAGSHUB_USER_TOKEN='${{ secrets.DAGSHUB_USER_TOKEN }}' \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Deploy to EKS
        run: |
          kubectl apply -f deployment.yaml